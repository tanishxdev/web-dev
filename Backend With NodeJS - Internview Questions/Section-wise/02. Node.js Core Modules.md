# Node.js Core Modules

--- 

# **11. Describe some of the core modules of Node.js**

## **Concept**

Node.js comes with a set of **built-in core modules**, meaning you can use them **without installation**. These modules provide essential functionality for building servers, handling files, working with operating system features, and more.

Core modules are part of Node's runtime and are loaded using:

```js
const http = require("http");
```

They are extremely optimized and used everywhere in Node.js development.

---

# **Most Important Core Modules (Interview-Focused)**

Below are the **top core modules** you must know, with explanations and examples.

---

## **1. HTTP Module**

Used to create web servers and handle HTTP requests/responses.

```js
const http = require("http");

const server = http.createServer((req, res) => {
  res.end("Hello from HTTP module");
});

server.listen(3000);
```

---

## **2. FS (File System) Module**

Used for reading/writing files.

Supports both **blocking (sync)** and **non-blocking (async)** methods.

```js
const fs = require("fs");

fs.readFile("data.txt", "utf-8", (err, data) => {
  console.log(data);
});
```

---

## **3. Path Module**

Used to work with file and directory paths.

Helps avoid OS-specific issues.

```js
const path = require("path");

console.log(path.join(__dirname, "folder", "file.txt"));
```

---

## **4. Events Module**

Used for creating and handling custom events.

```js
const EventEmitter = require("events");

const emitter = new EventEmitter();
emitter.on("start", () => console.log("Event started"));
emitter.emit("start");
```

---

## **5. OS Module**

Provides information about the operating system.

```js
const os = require("os");

console.log(os.platform());
console.log(os.totalmem());
```

---

## **6. Crypto Module**

Used for hashing, encryption, HMAC, etc.

```js
const crypto = require("crypto");

const hash = crypto.createHash("sha256")
  .update("password")
  .digest("hex");

console.log(hash);
```

---

## **7. Stream Module**

Streams allow data to be processed piece-by-piece, not all at once.

Example: reading large files

```js
const fs = require("fs");

const stream = fs.createReadStream("large.txt");
stream.on("data", chunk => console.log("Received chunk"));
```

---

## **8. URL Module**

Used to parse and work with URLs.

```js
const { URL } = require("url");

const url = new URL("https://example.com/product?id=100");
console.log(url.searchParams.get("id"));
```

---

## **9. QueryString Module**

Parses URL query parameters.

```js
const qs = require("querystring");

console.log(qs.parse("id=5&name=tanish"));
```

---

## **10. Child Process Module**

Used to create new processes.

```js
const { exec } = require("child_process");

exec("ls", (err, stdout) => {
  console.log(stdout);
});
```

---

# **Short Interview Answer**

Node.js provides built-in core modules like `http` for creating servers, `fs` for file operations, `path` for handling file paths, `events` for event handling, `os` for system information, `crypto` for encryption, and `stream` for streaming data. These modules come pre-installed and offer essential functionality for Node.js applications.

---

# **Summary Table**

| Module        | Purpose                         |
| ------------- | ------------------------------- |
| http          | Web server creation             |
| fs            | File read/write                 |
| path          | File path operations            |
| events        | Custom event handling           |
| os            | System/OS info                  |
| crypto        | Hashing & encryption            |
| stream        | Handling continuous data chunks |
| url           | URL parsing                     |
| querystring   | Parse/query parameters          |
| child_process | Run subprocesses                |

---


# **12. How do you create a simple server in Node.js using the HTTP module?**

## **Concept**

The **HTTP module** is one of the core modules in Node.js and allows you to:

* create a basic web server
* handle incoming requests (req)
* send responses (res)
* listen on a specific port

A server created with the HTTP module is **lightweight**, fast, and completely non-blocking.

---

# **How the HTTP Server Works Internally**

1. Import the http module
2. Create a server using `http.createServer()`
3. Listen for incoming requests
4. Send responses
5. Bind the server to a port (e.g., 3000)

---

# **Basic Example: Simple HTTP Server**

```js
// server.js
const http = require("http");

// Create server
const server = http.createServer((req, res) => {
  res.writeHead(200, { "Content-Type": "text/plain" });
  res.end("Hello from Node.js HTTP server");
});

// Start server on port 3000
server.listen(3000, () => {
  console.log("Server running at http://localhost:3000");
});
```

---

# **What Happens Here (Step-by-Step)**

### **1. http.createServer()**

Creates a server instance.

### **2. Callback function (req, res)**

Runs every time someone hits the server.

### **3. res.writeHead()**

Sets status code & headers.

### **4. res.end()**

Sends final response to client.

### **5. server.listen(3000)**

Server is ready to accept requests on port 3000.

---

# **Handling Different Routes (Small Improvement)**

```js
const http = require("http");

const server = http.createServer((req, res) => {
  if (req.url === "/") {
    res.end("Home Page");
  } else if (req.url === "/about") {
    res.end("About Page");
  } else {
    res.writeHead(404);
    res.end("Page Not Found");
  }
});

server.listen(3000);
```

---

# **Serving JSON Response**

```js
const http = require("http");

const server = http.createServer((req, res) => {
  res.writeHead(200, { "Content-Type": "application/json" });
  res.end(JSON.stringify({ message: "API Response" }));
});

server.listen(3000);
```

---

# **Short Interview Answer**

The HTTP module allows you to create a web server in Node.js. You import the http module, create a server using `http.createServer()`, define how it handles requests and responses, and call `server.listen()` to bind it to a port. This server can handle multiple concurrent requests using Node’s event-driven architecture.

---

# **Summary Table**

| Step           | Description                           |
| -------------- | ------------------------------------- |
| Import http    | `const http = require("http")`        |
| Create server  | `http.createServer((req, res) => {})` |
| Handle req/res | Use routes, writeHead, end            |
| Start server   | `server.listen(3000)`                 |

---

# **13. Explain the purpose of the File System (fs) module**

## **Concept**

The **File System (fs)** module is a core Node.js module that provides an API for interacting with the file system.

It allows you to:

* read files
* write files
* update files
* delete files
* create directories
* watch for file changes

The fs module supports both:

1. **Synchronous (blocking)** methods
2. **Asynchronous (non-blocking)** methods → recommended

---

# **Why Node.js Needs the FS Module**

Node.js is widely used for:

* web servers
* file upload systems
* logging systems
* configuration readers
* CLI tools
* scripts
* data pipelines

All these require file access. The `fs` module enables this using **fast, non-blocking I/O**.

---

# **Most Common FS Operations (Interview-Focused)**

## **1. Reading a File (Non-blocking)**

```js
const fs = require("fs");

fs.readFile("data.txt", "utf-8", (err, data) => {
  if (err) throw err;
  console.log(data);
});
```

This does not block the main thread.

---

## **2. Reading a File Synchronously (Blocking)**

```js
const fs = require("fs");

const data = fs.readFileSync("data.txt", "utf-8");
console.log(data);
```

Blocks execution until the file is fully read.

---

## **3. Writing to a File**

```js
fs.writeFile("output.txt", "Hello World", (err) => {
  if (err) throw err;
  console.log("File written");
});
```

If file does not exist → creates it.

---

## **4. Appending to a File**

```js
fs.appendFile("log.txt", "New log entry\n", (err) => {
  if (err) throw err;
});
```

---

## **5. Deleting a File**

```js
fs.unlink("temp.txt", (err) => {
  if (err) throw err;
});
```

---

## **6. Creating a Directory**

```js
fs.mkdir("data", (err) => {
  if (err) throw err;
});
```

---

## **7. Watching File Changes**

```js
fs.watch("data.txt", () => {
  console.log("File changed");
});
```

Useful for log monitoring or file watchers.

---

# **Important Internal Detail**

The fs module uses:

**libuv thread pool**
to offload slow file operations so that Node.js main thread stays non-blocking.

This is often discussed in interviews.

---

# **Common Use Cases**

* reading configuration files
* writing logs
* saving user uploads
* reading templates
* exporting data to .txt/.json
* backend cron jobs

---

# **Short Interview Answer**

The fs module allows Node.js applications to interact with the file system. It provides both synchronous and asynchronous methods for reading, writing, updating, deleting, and watching files and directories. Internally, fs operations use the libuv thread pool, keeping the main thread non-blocking.

---

# **Summary Table**

| Operation       | Method            |
| --------------- | ----------------- |
| Read file async | `fs.readFile`     |
| Read file sync  | `fs.readFileSync` |
| Write file      | `fs.writeFile`    |
| Append file     | `fs.appendFile`   |
| Delete file     | `fs.unlink`       |
| Make directory  | `fs.mkdir`        |
| Watch file      | `fs.watch`        |

---

# **14. What is the Buffer class in Node.js?**

## **Concept**

A **Buffer** in Node.js is a raw binary data container.
It is used to handle **binary data directly**, especially when dealing with:

* file reading
* network protocols
* streaming data
* binary formats (images, videos, PDFs)

Because JavaScript originally had **no binary type**, Node.js introduced Buffer to work efficiently with binary data.

Buffers are stored outside the V8 JavaScript engine in raw memory.

---

# **Why Buffer Exists**

JavaScript strings cannot represent binary data properly.
Servers often need to handle:

* file chunks
* network packets
* audio/video streams
* encryption data

Buffer gives Node.js the ability to process binary data like low-level languages (C/C++).

---

# **Key Features of Buffer**

1. **Fixed-size**
2. **Represents binary data as bytes**
3. **Direct memory allocation**
4. **Fast and efficient**
5. **Works with streams**

---

# **Creating Buffers**

## **1. From a string**

```js
const buf = Buffer.from("Hello");
console.log(buf);
```

Output (bytes):

```
<Buffer 48 65 6c 6c 6f>
```

---

## **2. Allocate empty buffer**

```js
const buf = Buffer.alloc(10);   // 10 bytes
console.log(buf);
```

---

## **3. Allocate an unsafe buffer**

```js
const buf = Buffer.allocUnsafe(10);
```

Faster, but contains old memory data until overwritten.

---

# **Working with Buffers**

## **Write to buffer**

```js
const buf = Buffer.alloc(5);
buf.write("Hello");
console.log(buf.toString());
```

---

## **Read buffer content**

```js
console.log(buf.toString("utf-8"));
```

---

## **Get byte length**

```js
Buffer.byteLength("Hello");
```

---

## **Modify buffer**

```js
buf[0] = 72;  // ASCII for 'H'
```

---

# **Buffers with Streams (Most Important)**

When reading a file using streams:

```js
const fs = require("fs");

const stream = fs.createReadStream("image.png");

stream.on("data", chunk => {
  console.log("Chunk type:", chunk);
});
```

Each `chunk` is a Buffer.

This is how Node.js reads large files **without loading entire file into memory**.

---

# **Buffers with Network Calls**

TCP sockets, WebSockets, and HTTP streaming all use Buffers to exchange raw bytes.

---

# **Short Interview Answer**

The Buffer class in Node.js represents raw binary data. It allows Node.js to handle files, network packets, and streaming data efficiently. A Buffer is a fixed-size chunk of memory that stores bytes directly and is widely used with streams and low-level operations.

---

# **Summary Table**

| Feature         | Description                             |
| --------------- | --------------------------------------- |
| Purpose         | Handle binary data                      |
| Memory location | Outside V8 heap                         |
| Used for        | Files, streams, network packets         |
| Methods         | Buffer.from, Buffer.alloc, Buffer.write |
| Type            | Fixed-size byte array                   |

---

# **15. What are streams in Node.js and what types are available?**

## **Concept**

A **stream** in Node.js is a continuous flow of data that is processed **piece-by-piece**, rather than loading everything into memory at once.

This makes streams ideal for:

* reading large files
* uploading or downloading data
* audio/video streaming
* network communication
* real-time data processing

Streams help Node.js remain **fast and memory-efficient**.

---

# **Why Streams Are Needed**

Imagine reading a 2GB file:

* Without streams → Node must load entire file in memory → crash or slow
* With streams → Node reads it **chunk by chunk**

This is why streams are essential for backend performance.

---

# **Types of Streams in Node.js**

Node.js has **4 main types** of streams:

---

## **1. Readable Streams**

These streams **provide data** to your application.

Examples:

* `fs.createReadStream()`
* HTTP request body
* process.stdin

Example:

```js
const fs = require("fs");

const stream = fs.createReadStream("input.txt");

stream.on("data", chunk => {
  console.log("Chunk received:", chunk.toString());
});
```

---

## **2. Writable Streams**

These streams **receive data** from your application.

Examples:

* file writes (`fs.createWriteStream()`)
* HTTP response
* process.stdout

Example:

```js
const fs = require("fs");

const stream = fs.createWriteStream("output.txt");

stream.write("Hello World");
stream.end();
```

---

## **3. Duplex Streams**

Both **readable + writable** versions in one stream.

Example:

* TCP sockets
* WebSockets

```js
const net = require("net");

const socket = net.Socket();
// socket is both readable and writable
```

---

## **4. Transform Streams**

A special type of Duplex stream where **output is transformed version of input**.

Examples:

* zlib compression
* crypto hashing
* converting data formats

Example:

```js
const { Transform } = require("stream");

const upperCase = new Transform({
  transform(chunk, encoding, callback) {
    callback(null, chunk.toString().toUpperCase());
  }
});
```

---

# **Why Streams Are Powerful**

### **1. Memory Efficient**

Processes data in chunks.

### **2. Faster**

Stream processing begins immediately without waiting for full data.

### **3. Composable**

You can pipe streams to each other.

---

# **Piping Example (Most Important)**

```js
const fs = require("fs");

fs.createReadStream("input.txt")
  .pipe(fs.createWriteStream("output.txt"));
```

This copies file efficiently, chunk-by-chunk.

---

# **Streams in Real Systems**

Used in:

* video/audio streaming platforms
* chat applications
* file upload & download APIs
* log processing
* proxy servers

---

# **Short Interview Answer**

Streams in Node.js allow handling of continuous data efficiently by processing it in chunks instead of loading it all at once. There are four types of streams: Readable, Writable, Duplex, and Transform. Streams are widely used for files, network operations, and real-time applications.

---

# **Summary Table**

| Stream Type | Purpose                       | Examples                            |
| ----------- | ----------------------------- | ----------------------------------- |
| Readable    | Provide data                  | fs.createReadStream, HTTP request   |
| Writable    | Consume data                  | fs.createWriteStream, HTTP response |
| Duplex      | Read + Write                  | TCP sockets                         |
| Transform   | Modify data during read/write | zlib, crypto hash                   |

---

# **16. How do you read and write files in Node.js?**

## **Concept**

Reading and writing files in Node.js is done using the **fs (File System) module**, which supports:

1. **Asynchronous (non-blocking) methods** → recommended
2. **Synchronous (blocking) methods** → avoid in servers
3. **Streaming methods** → best for large files

Understanding these is crucial in interviews because file I/O is a core Node.js use case.

---

# **A. Reading Files (Asynchronous – Recommended)**

```js
const fs = require("fs");

fs.readFile("data.txt", "utf-8", (err, data) => {
  if (err) throw err;
  console.log(data);
});
```

### Why use this?

Because it does **not block the main thread**.

---

# **B. Reading Files Synchronously (Blocking)**

```js
const fs = require("fs");

const data = fs.readFileSync("data.txt", "utf-8");
console.log(data);
```

### Why avoid?

It **blocks** execution until the file is completely read.

Useful only for:

* small scripts
* configuration loading before server starts

---

# **C. Writing Files (Asynchronous)**

```js
const fs = require("fs");

fs.writeFile("output.txt", "Hello World", (err) => {
  if (err) throw err;
  console.log("File written");
});
```

If file doesn’t exist → Node creates it.

---

# **D. Writing Files Synchronously (Blocking)**

```js
const fs = require("fs");

fs.writeFileSync("output.txt", "Hello World");
```

Blocks until writing is done.

---

# **E. Appending Data to a File**

```js
fs.appendFile("log.txt", "New entry\n", (err) => {
  if (err) throw err;
});
```

Used for:

* logs
* activity tracking

---

# **F. Deleting a File**

```js
fs.unlink("temp.txt", (err) => {
  if (err) throw err;
});
```

---

# **G. Reading Large Files with Streams (Best for Big Files)**

```js
const fs = require("fs");

const stream = fs.createReadStream("largefile.txt", "utf-8");

stream.on("data", chunk => {
  console.log("Received chunk:", chunk);
});
```

### Why?

Streams process data **piece by piece**, avoiding memory overflow.

---

# **H. Writing Large Files with Streams**

```js
const fs = require("fs");

const stream = fs.createWriteStream("output.txt");

stream.write("Hello\n");
stream.write("World\n");
stream.end();
```

---

# **I. Copying Files Efficiently Using Streams**

```js
fs.createReadStream("input.txt")
  .pipe(fs.createWriteStream("output.txt"));
```

Extremely efficient due to **pipe()**.

---

# **Short Interview Answer**

Node.js reads and writes files using the fs module. It provides both asynchronous (`fs.readFile`, `fs.writeFile`) and synchronous (`fs.readFileSync`, `fs.writeFileSync`) methods. Async methods are preferred because they are non-blocking. For large files, streams (`createReadStream`, `createWriteStream`) are used to process data in chunks.

---

# **Summary Table**

| Task        | Async Method    | Sync Method         | Notes                   |
| ----------- | --------------- | ------------------- | ----------------------- |
| Read file   | `fs.readFile`   | `fs.readFileSync`   | Async recommended       |
| Write file  | `fs.writeFile`  | `fs.writeFileSync`  | Creates file if missing |
| Append file | `fs.appendFile` | `fs.appendFileSync` | Good for logs           |
| Delete file | `fs.unlink`     | `fs.unlinkSync`     | Removes file            |
| Large files | Streams         | —                   | Best performance        |

---

# **17. How do you use the EventEmitter in Node.js?**

## **Concept**

`EventEmitter` is a core class in Node.js used to implement **event-driven programming**.

It allows you to:

* create custom events
* emit events
* listen for events
* pass data with events

Internally, Node.js itself uses EventEmitter everywhere:

* HTTP server (`req`, `res`)
* Streams
* Sockets
* Process events
* Timers

So understanding EventEmitter is a must for interviews.

---

# **How EventEmitter Works**

There are **three main steps**:

1. **Import EventEmitter**
2. **Register a listener** (using `.on()` or `.once()`)
3. **Emit an event** (using `.emit()`)

---

# **Basic Example**

```js
const EventEmitter = require("events");

// Create emitter instance
const emitter = new EventEmitter();

// Listener
emitter.on("greet", (name) => {
  console.log("Hello", name);
});

// Emit event
emitter.emit("greet", "Tanish");
```

### Explanation:

* `.on()` → listens for event
* `.emit()` → triggers event
* Listener receives `"Tanish"` as argument

---

# **Using .once() – Runs only one time**

```js
emitter.once("start", () => {
  console.log("This runs only once");
});

emitter.emit("start");
emitter.emit("start");   // Will not run
```

---

# **Passing Multiple Arguments**

```js
emitter.on("order", (item, quantity) => {
  console.log(`Ordered ${quantity} x ${item}`);
});

emitter.emit("order", "Shoes", 2);
```

---

# **Removing Listeners**

### **Remove a specific listener**

```js
function handler() {
  console.log("Event triggered");
}

emitter.on("test", handler);

// Remove it
emitter.removeListener("test", handler);
```

### **Remove all listeners**

```js
emitter.removeAllListeners("test");
```

---

# **Error Events (Very Important)**

If an error event is emitted but not handled → Node.js will crash.

```js
emitter.on("error", (err) => {
  console.log("Error occurred:", err.message);
});

emitter.emit("error", new Error("Something failed"));
```

---

# **Real-World Example: Server Request Event**

```js
const http = require("http");

const server = http.createServer();

server.on("request", (req, res) => {
  res.end("Request received");
});

server.listen(3000);
```

Here, `request` is an event emitted by the http server.

---

# **EventEmitter Behind the Scenes**

* Maintains an internal event-listener map
* When `.emit()` is called → all handlers for that event are executed
* Order = FIFO (first registered → first called)

---

# **Short Interview Answer**

EventEmitter is a core Node.js class that allows creation and handling of custom events. You register listeners using `.on()` or `.once()`, and trigger events using `.emit()`. It’s the foundation of Node’s event-driven architecture and is used internally by streams, HTTP, and many other modules.

---

# **Summary Table**

| Method                   | Purpose                  |
| ------------------------ | ------------------------ |
| `.on(event, listener)`   | Listen for event         |
| `.once(event, listener)` | Listen only once         |
| `.emit(event, data)`     | Trigger event            |
| `.removeListener`        | Remove specific listener |
| `.removeAllListeners`    | Remove all listeners     |

---

# **18. What is the QueryString module?**

## **Concept**

The **QueryString module** is a built-in Node.js module used to:

* parse URL query strings into JavaScript objects
* convert objects back into URL query strings

It is mainly used for **working with URL parameters**.

Example URL:

```
https://example.com/product?id=10&name=shoes
```

Query string part:

```
id=10&name=shoes
```

Node.js can parse this into an object using `querystring`.

---

# **Why QueryString Exists**

Before the modern `URLSearchParams` API, servers needed an easy way to:

* read query parameters
* manipulate them
* build URL strings programmatically

`querystring` provides this support.

Although newer apps prefer `URLSearchParams`, `querystring` is still widely used and asked in interviews.

---

# **Basic Functions of querystring Module**

Import:

```js
const querystring = require("querystring");
```

---

# **1. Parsing a Query String → Object**

```js
const qs = require("querystring");

const result = qs.parse("id=10&name=tanish");

console.log(result);
```

Output:

```js
{ id: '10', name: 'tanish' }
```

---

# **2. Stringifying an Object → Query String**

```js
const qs = require("querystring");

const result = qs.stringify({ id: 10, name: "tanish" });

console.log(result);
```

Output:

```
id=10&name=tanish
```

---

# **3. Custom Separator and Assignment Characters**

```js
qs.parse("id:10|name:tanish", "|", ":");
```

Output:

```js
{ id: '10', name: 'tanish' }
```

---

# **Difference Between querystring and URLSearchParams**

| Feature           | querystring | URLSearchParams |
| ----------------- | ----------- | --------------- |
| Built-in          | Yes         | Yes             |
| Older API         | Yes         | No              |
| Recommended       | No          | Yes             |
| Supports encoding | Yes         | Yes             |

**Modern Node.js recommends using:**

```js
new URLSearchParams(req.url)
```

but querystring is still part of core modules and often asked in interviews.

---

# **Real Use Case: Parsing URL Parameters in a Server**

```js
const http = require("http");
const qs = require("querystring");

const server = http.createServer((req, res) => {
  const query = req.url.split("?")[1];
  const params = qs.parse(query);

  console.log(params);

  res.end("Parsed");
});

server.listen(3000);
```

---

# **Short Interview Answer**

The QueryString module is a core Node.js module used to parse URL query strings into JavaScript objects and to convert objects back into query strings. It provides simple utilities for handling URL parameters, although modern Node.js prefers URLSearchParams.

---

# **Summary Table**

| Method            | Purpose                        |
| ----------------- | ------------------------------ |
| `qs.parse()`      | Converts query string → object |
| `qs.stringify()`  | Converts object → query string |
| Separator support | Allows custom separators       |

---

# **19. How do you manage path operations in Node.js?**



## **Concept**

Node.js provides a core module called **path** that helps you work with:

* file paths
* directory paths
* extensions
* OS-independent path handling

File paths differ between OS:

```
Windows → C:\user\file.txt  
Linux/Mac → /user/file.txt
```

The **path module normalizes everything**, ensuring consistent behavior.

---

# **Why path Module Is Needed**

1. Avoid OS-specific issues
2. Avoid manually joining paths
3. Prevent errors when building file systems
4. Resolve absolute vs relative paths
5. Extract file extensions, directories, filenames

This is extremely important in backend systems dealing with:

* uploads
* logs
* routing
* static file serving

---

# **Importing the Module**

```js
const path = require("path");
```

---

# **Important path Methods (Interview Focus)**

---

## **1. path.join()**

Joins paths safely (resolves extra slashes).

```js
path.join(__dirname, "folder", "file.txt");
```

Output:

```
/current/dir/folder/file.txt
```

---

## **2. path.resolve()**

Returns an **absolute path**.

```js
path.resolve("folder", "file.txt");
```

Output example:

```
/Users/Tanish/project/folder/file.txt
```

---

## **3. path.basename()**

Returns the file name.

```js
path.basename("/user/data/info.txt");
```

Output:

```
info.txt
```

---

## **4. path.dirname()**

Returns directory path.

```js
path.dirname("/user/data/info.txt");
```

Output:

```
/user/data
```

---

## **5. path.extname()**

Returns file extension:

```js
path.extname("image.png");
```

Output:

```
.png
```

---

## **6. path.parse()**

Breaks the path into an object.

```js
console.log(path.parse("/user/home/file.txt"));
```

Output:

```json
{
  "root": "/",
  "dir": "/user/home",
  "base": "file.txt",
  "ext": ".txt",
  "name": "file"
}
```

---

## **7. path.normalize()**

Fixes incorrect path formatting.

```js
path.normalize("/user//home/../file.txt");
```

Output:

```
/user/file.txt
```

---

# **Real Example: Serving Static Files**

```js
const fs = require("fs");
const path = require("path");

const filePath = path.join(__dirname, "public", "index.html");

fs.readFile(filePath, "utf-8", (err, data) => {
  console.log(data);
});
```

This works reliably on Windows, Linux, and Mac.

---

# **Absolute vs Relative Paths**

### **__dirname**

Directory of current file

### **__filename**

Full path of current file

Example:

```js
console.log(__dirname);
console.log(__filename);
```

Very important in servers.

---

# **Short Interview Answer**

Node.js manages path operations using the built-in path module. It provides utilities like `path.join()` for joining paths, `path.resolve()` for absolute paths, `path.basename()` for filenames, and `path.extname()` for extensions. The module ensures OS-independent, error-free path handling.

---

# **Summary Table**

| Method    | Purpose                    |
| --------- | -------------------------- |
| join      | Joins paths safely         |
| resolve   | Creates absolute path      |
| basename  | Extract filename           |
| dirname   | Extract directory          |
| extname   | Extract extension          |
| parse     | Break full path into parts |
| normalize | Clean messy paths          |

---
