# Topic 28 Node.js Streams (Readable, Writable, Duplex, Transform)

---

### **Concept**

**What are Streams?**

Streams allow Node.js to work with **large data chunks piece-by-piece**, instead of loading entire data into memory.
This makes streams highly efficient for:

* Large file reading/writing
* Video/Audio streaming
* Network packets
* Data pipes (compress, encrypt, transform)

---

### **Why Streams?**

| Issue (Without Streams)             | Streams Benefit                    |
| ----------------------------------- | ---------------------------------- |
| Reading a 5GB file requires 5GB RAM | Streams read data in **chunks**    |
| Slow data processing                | Process data **as it arrives**     |
| Memory overflow risk                | Constant memory usage              |
| Cannot handle large uploads         | Streams handle **chunked uploads** |

---

### **Types of Streams in Node.js**

| Type          | Description                                              |
| ------------- | -------------------------------------------------------- |
| **Readable**  | Data can be read from the stream                         |
| **Writable**  | Data can be written to the stream                        |
| **Duplex**    | Read and write both (e.g., TCP socket)                   |
| **Transform** | Data can be modified while streaming (e.g., compression) |

---

### **Events on Streams**

| Event    | Trigger                         |
| -------- | ------------------------------- |
| `data`   | When chunk of data is available |
| `end`    | No more data                    |
| `error`  | Error occurs                    |
| `finish` | Writable stream done writing    |

---

## 1) Readable Stream Example (Read File in Chunks)

**File:** `readStream.js`

```js
const fs = require("fs");

const stream = fs.createReadStream("bigfile.txt", { encoding: "utf8" });

stream.on("data", chunk => {
  console.log("Chunk received:");
  console.log(chunk);
});

stream.on("end", () => {
  console.log("Finished reading file");
});

stream.on("error", err => {
  console.log("Error:", err);
});
```

**Run:**

```bash
node readStream.js
```

---

## 2) Writable Stream Example (Write File in Chunks)

**File:** `writeStream.js`

```js
const fs = require("fs");

const stream = fs.createWriteStream("output.txt");

stream.write("Hello ");
stream.write("Stream ");
stream.write("World!\n");

stream.end(() => {
  console.log("Finished writing");
});
```

---

## 3) Pipe (Directly connecting streams)

**Copy file using pipe:**

```js
const fs = require("fs");

fs.createReadStream("bigfile.txt")
  .pipe(fs.createWriteStream("copy.txt"));
```

This copies file without buffering entire file in memory.

---

## 4) Duplex Stream Example (TCP Chat)

Duplex streams read/write both ways.

**server.js**

```js
const net = require('net');

const server = net.createServer(socket => {
  socket.write("Welcome to TCP chat!\n");

  socket.on("data", data => {
    socket.write(`You said: ${data}`);
  });
});

server.listen(5000, () => console.log("TCP Server running on 5000"));
```

Connect using terminal:

```bash
nc localhost 5000
```

---

## 5) Transform Stream Example (Compression)

Compress file with **zlib**:

```js
const fs = require("fs");
const zlib = require("zlib");

fs.createReadStream("bigfile.txt")
  .pipe(zlib.createGzip())
  .pipe(fs.createWriteStream("bigfile.txt.gz"));
```

Decompress:

```js
fs.createReadStream("bigfile.txt.gz")
  .pipe(zlib.createGunzip())
  .pipe(fs.createWriteStream("uncompressed.txt"));
```

---

## Mini Project: HTTP Streaming Server (Video Streaming)

**File:** `videoServer.js`

```js
const http = require("http");
const fs = require("fs");

const server = http.createServer((req, res) => {
  const stream = fs.createReadStream("video.mp4");
  res.writeHead(200, { "Content-Type": "video/mp4" });
  stream.pipe(res);
});

server.listen(4000, () => console.log("Server streaming on http://localhost:4000"));
```

Open browser:
`http://localhost:4000` â†’ Video will stream instead of full download.

---

## Streams + Buffer Relationship

| Buffer                | Stream                      |
| --------------------- | --------------------------- |
| holds raw binary data | processes Buffers in chunks |
| stored in memory      | forwarded chunk-by-chunk    |
| static size           | dynamic flow                |

Streams are like pipelines, Buffers are like buckets of data.

---

## Dependencies

| Module   | Built-in |
| -------- | -------- |
| `fs`     | Yes      |
| `zlib`   | Yes      |
| `stream` | Yes      |
| `net`    | Yes      |
| `http`   | Yes      |

No external package required.

---

## Notes

* Always handle `error` events, or Node may crash.
* Use `pipe()` for chaining streams efficiently.
* Streams prevent memory overflow for large files.
* Great for uploading/downloading large content.
* Transform streams help in compression, encryption, data formatting, etc.

---

### Quick Summary Table

| Stream Type   | Example                   |
| ------------- | ------------------------- |
| **Readable**  | `fs.createReadStream()`   |
| **Writable**  | `fs.createWriteStream()`  |
| **Duplex**    | `net.Socket`              |
| **Transform** | `zlib.createGzip()`       |
| **Pipe**      | `readable.pipe(writable)` |
